{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiyan Liu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gzip\n",
    "from transformers import pipeline\n",
    "import torch.nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "trainDF=pd.read_csv(\"data\\\\train.csv\")\n",
    "\n",
    "# Validation Set\n",
    "validationDF=pd.read_csv(\"data\\\\validation.csv\")\n",
    "\n",
    "# Testing Set\n",
    "testDF=pd.read_csv(\"data\\\\test.csv\")\n",
    "\n",
    "# Full Data Set\n",
    "fullDF=pd.read_csv(\"data\\\\data.csv\")\n",
    "\n",
    "EMOTIONSDICT={\"sadness\":0, \"joy\":1, \"love\":2, \"anger\":3, \"fear\":4, \"surprise\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.vocabulary={} #Set that contains all vocabulary words in our training set.\n",
    "        self.sentences=self.createSparseMat(df) #Tokenized representation of all sentences in dataframe.\n",
    "        self.targets=self.createTargets(df) #Targets for each sentence, corresponding to the labelled emotions.\n",
    "\n",
    "        self.numSamples=self.sentences.shape[0] #Number of sentences, N.\n",
    "        self.numFeatures=self.sentences.shape[1] #Number of words in our vocabulary, D.\n",
    "        self.numTargets=max(self.targets)+1 #Number of different sentiments, C.g\n",
    "        self.labelledSentences=self.createLabelledSentences() #Dictioanry, CxN_cxD, where each key represents\n",
    "            #an emotion and holds an array of all sentences that are labelled that emotion.\n",
    "        self.wordFrequencies=self.createWordFrequencies() #CxD dimensional array, where each array is the sum\n",
    "            #of the d-th word across all sentences of class c.\n",
    "        self.totalWordFrequencies=self.createTotalWordFrequencies() #C dimensional array, where each index is the \n",
    "            #sum of all words in sentences of class c.\n",
    "\n",
    "        self.priorProbabilities=None\n",
    "        self.posteriorProbabilities=None\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\":self.sentences[idx],\n",
    "            'emotions': self.targets[idx]\n",
    "        }\n",
    "        \n",
    "    def createWordFrequencies(self):\n",
    "        res=[0]*self.numTargets\n",
    "        for c in range(self.numTargets):\n",
    "            res[c]=np.sum(self.labelledSentences[c], axis=0)\n",
    "        return res\n",
    "    \n",
    "    def createTotalWordFrequencies(self):\n",
    "        res=[0]*self.numTargets\n",
    "        for c in range(self.numTargets):\n",
    "            res[c]=np.sum(self.labelledSentences[c])\n",
    "        return res\n",
    "    \n",
    "    def createLabelledSentences(self):\n",
    "        res=dict.fromkeys([0,1,2,3,4,5,6])\n",
    "        for key in res:\n",
    "            res[key]=[]\n",
    "        for i in range(len(self.sentences)):\n",
    "            target=self.targets[i]\n",
    "            res[target].append(self.sentences[i])\n",
    "        return res\n",
    "    \n",
    "    #Returns a bag of words matrix representation of each sentence in our dataset.\n",
    "    def createSparseMat(self, df, test=0):\n",
    "        \n",
    "        phrases=[]\n",
    "        for i in range(len(df)):\n",
    "            sample=df.loc[i, \"text\"]\n",
    "            phrases.append(sample)\n",
    "            \n",
    "        #If we're testing, we need to use the vocabulary from our training set to make the sparse matrix.\n",
    "        vectorizer=None\n",
    "        if test:\n",
    "            vectorizer=CountVectorizer(vocabulary=self.vocabulary)\n",
    "            sparseMat=vectorizer.fit_transform(phrases)\n",
    "            return sparseMat.toarray()\n",
    "        else:\n",
    "            vectorizer=CountVectorizer()\n",
    "            sparseMat=vectorizer.fit_transform(phrases)\n",
    "            self.vocabulary=vectorizer.vocabulary_\n",
    "            return sparseMat.toarray()\n",
    "    \n",
    "    #Creates corresponding targets to each sentence.\n",
    "    def createTargets(self,df):\n",
    "        labels=[]\n",
    "        for i in range(len(df)):\n",
    "            labels.append(df.loc[i,\"emotions\"])\n",
    "        return labels\n",
    "    \n",
    "    #Creates and stores parameters as model attributes.\n",
    "    def fit(self):\n",
    "        priorProbabilities=self.createPriorProbabilities()\n",
    "        self.priorProbabilities=priorProbabilities\n",
    "        \n",
    "        posteriorProbabilities=self.createPosteriorProbabilities()\n",
    "        self.posteriorProbabilities=posteriorProbabilities\n",
    "        \n",
    "    #Creates prior probabilities of each emotion using multinoulli classification. \n",
    "    def createPriorProbabilities(self):\n",
    "        priorProbabilities=[0]*self.numTargets\n",
    "        for i in range(len(priorProbabilities)):\n",
    "            count=0\n",
    "            for target in self.targets:\n",
    "                if int(target)==i:\n",
    "                    count+=1\n",
    "            priorProbabilities[i]=count/self.numSamples\n",
    "        return priorProbabilities\n",
    "\n",
    "    #Creates posterior probabilites, the theta_(d,c).\n",
    "    def createPosteriorProbabilities(self):\n",
    "        posteriorProbabilites=[[0]*self.numFeatures for d in range(self.numTargets)] #CxD\n",
    "        \n",
    "        for c in range(self.numTargets):\n",
    "            posteriorProbabilites[c]=np.divide(self.wordFrequencies[c], self.totalWordFrequencies[c])\n",
    "        \n",
    "        return posteriorProbabilites\n",
    "    \n",
    "    #Predicts the labels for a test/validation dataframe.\n",
    "    def predict(self, testdf):\n",
    "        testMatrix=self.createSparseMat(testdf, test=1)\n",
    "        predictedTargets=[]\n",
    "        \n",
    "        for i in range(len(testMatrix)):\n",
    "            sentence=testMatrix[i]\n",
    "            probabilities=self.predictSentence(sentence)\n",
    "            predictedTargets.append(probabilities.index(max(probabilities)))\n",
    "            \n",
    "        return predictedTargets\n",
    "\n",
    "    #Gets accuracy for a test dataframe.\n",
    "    def getAcc(self, testdf):\n",
    "        testTargets=self.createTargets(testdf)\n",
    "        predictedTargets=self.predict(testdf)\n",
    "        \n",
    "        right=0\n",
    "        for i in range(len(testTargets)):\n",
    "            if testTargets[i]==predictedTargets[i]:\n",
    "                right+=1\n",
    "        return right/len(testTargets)\n",
    "    \n",
    "    def normalizationFactor(self,sentence):\n",
    "        sentenceSum=sum(sentence)\n",
    "        downstairs=1\n",
    "        for freq in sentence:\n",
    "            downstairs*=np.math.factorial(freq)\n",
    "        return np.math.factorial(sentenceSum)/downstairs\n",
    "    \n",
    "    #Returns probability of sentence being each class c.\n",
    "    def predictSentence(self, sentence):\n",
    "        probabilities=[0]*self.numTargets\n",
    "        \n",
    "        for c in range(self.numTargets):\n",
    "            res=1\n",
    "            res*=self.priorProbabilities[c] #Probability of it being class c.\n",
    "            for d in range(self.numFeatures):\n",
    "                factor=self.posteriorProbabilities[c][d]**(sentence[d])\n",
    "                res*=(factor) #Posterior probabilities.\n",
    "            probabilities[c]=res\n",
    "            \n",
    "        pSum=sum(probabilities)\n",
    "        for i in range(len(probabilities)):\n",
    "            probabilities[i]=probabilities[i]/pSum\n",
    "\n",
    "        return probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiyan Liu\\AppData\\Local\\Temp\\ipykernel_13520\\3833645821.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  probabilities[i]=probabilities[i]/pSum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.62\n",
      "Test accuracy is: 0.643\n"
     ]
    }
   ],
   "source": [
    "bayesModel=NaiveBayes(trainDF) #Training model\n",
    "testModel=NaiveBayes(testDF) #Easy access to processed data\n",
    "\n",
    "\n",
    "bayesModel.fit()\n",
    "\n",
    "print(\"Validation accuracy is:\", bayesModel.getAcc(validationDF))\n",
    "print(\"Test accuracy is:\", bayesModel.getAcc(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9265\n"
     ]
    }
   ],
   "source": [
    "#Importing pre-trained model without changing weights. \n",
    "bert = pipeline(\"sentiment-analysis\",model='bhadresh-savani/bert-base-uncased-emotion')\n",
    "\n",
    "right=0\n",
    "for i in range(len(testModel.sentences)):\n",
    "    sentence=testDF.loc[i,\"text\"]\n",
    "    prediction=bert(sentence)\n",
    "    predictedTarget=EMOTIONSDICT[prediction[0][\"label\"]]\n",
    "    if predictedTarget==testModel.targets[i]:\n",
    "        right+=1\n",
    "\n",
    "print(\"Accuracy is:\", right/len(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from pandatorch import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Creating the model and tokenizer.\n",
    "modelName='bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(modelName)\n",
    "model = BertForSequenceClassification.from_pretrained(modelName, num_labels=6)\n",
    "\n",
    "def tokenizeFunction(sentence):\n",
    "    return tokenizer(sentence, padding=\"max_length\", truncation=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, \"text\"]  # 'text' column\n",
    "        emotions = self.data.loc[idx, \"emotions\"]  # 'emotions' column\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs[\"input_ids\"].squeeze(),\n",
    "            'attention_mask': inputs[\"attention_mask\"].squeeze(),\n",
    "            'labels': int(emotions),\n",
    "        }\n",
    "\n",
    "#Tokenizing the training and text strings\n",
    "trainDataset=CustomDataset(trainDF, tokenizer)\n",
    "testDataset=CustomDataset(testDF, tokenizer)\n",
    "trainLoader=DataLoader(trainDataset, batch_size=16, shuffle=True)\n",
    "testLoader=DataLoader(testDataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Implementing pre-trained model, with changing weights.\n",
    "\n",
    "#Creating datasets for the training and text dataframes.\n",
    "trainingArgs= TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=trainDataset,\n",
    "    eval_dataset=testDataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
