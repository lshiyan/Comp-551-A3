{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiyan Liu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gzip\n",
    "from transformers import pipeline\n",
    "import torch.nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "trainDF=pd.read_csv(\"data\\\\train.csv\")\n",
    "\n",
    "# Validation Set\n",
    "validationDF=pd.read_csv(\"data\\\\validation.csv\")\n",
    "\n",
    "# Testing Set\n",
    "testDF=pd.read_csv(\"data\\\\test.csv\")\n",
    "\n",
    "# Full Data Set\n",
    "fullDF=pd.read_csv(\"data\\\\data.csv\")\n",
    "\n",
    "EMOTIONSDICT={\"sadness\":0, \"joy\":1, \"love\":2, \"anger\":3, \"fear\":4, \"surprise\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.vocabulary={} #Set that contains all vocabulary words in our training set.\n",
    "        self.sentences=self.createSparseMat(df) #Tokenized representation of all sentences in dataframe.\n",
    "        self.targets=self.createTargets(df) #Targets for each sentence, corresponding to the labelled emotions.\n",
    "\n",
    "        self.numSamples=self.sentences.shape[0] #Number of sentences, N.\n",
    "        self.numFeatures=self.sentences.shape[1] #Number of words in our vocabulary, D.\n",
    "        self.numTargets=max(self.targets)+1 #Number of different sentiments, C.g\n",
    "        self.labelledSentences=self.createLabelledSentences() #Dictioanry, CxN_cxD, where each key represents\n",
    "            #an emotion and holds an array of all sentences that are labelled that emotion.\n",
    "        self.wordFrequencies=self.createWordFrequencies() #CxD dimensional array, where each array is the sum\n",
    "            #of the d-th word across all sentences of class c.\n",
    "        self.totalWordFrequencies=self.createTotalWordFrequencies() #C dimensional array, where each index is the \n",
    "            #sum of all words in sentences of class c.\n",
    "\n",
    "        self.priorProbabilities=None\n",
    "        self.posteriorProbabilities=None\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\":self.sentences[idx],\n",
    "            'emotions': self.targets[idx]\n",
    "        }\n",
    "        \n",
    "    def createWordFrequencies(self):\n",
    "        res=[0]*self.numTargets\n",
    "        for c in range(self.numTargets):\n",
    "            res[c]=np.sum(self.labelledSentences[c], axis=0)\n",
    "        return res\n",
    "    \n",
    "    def createTotalWordFrequencies(self):\n",
    "        res=[0]*self.numTargets\n",
    "        for c in range(self.numTargets):\n",
    "            res[c]=np.sum(self.labelledSentences[c])\n",
    "        return res\n",
    "    \n",
    "    def createLabelledSentences(self):\n",
    "        res=dict.fromkeys([0,1,2,3,4,5,6])\n",
    "        for key in res:\n",
    "            res[key]=[]\n",
    "        for i in range(len(self.sentences)):\n",
    "            target=self.targets[i]\n",
    "            res[target].append(self.sentences[i])\n",
    "        return res\n",
    "    \n",
    "    #Returns a bag of words matrix representation of each sentence in our dataset.\n",
    "    def createSparseMat(self, df, test=0):\n",
    "        \n",
    "        phrases=[]\n",
    "        for i in range(len(df)):\n",
    "            sample=df.loc[i, \"text\"]\n",
    "            phrases.append(sample)\n",
    "            \n",
    "        #If we're testing, we need to use the vocabulary from our training set to make the sparse matrix.\n",
    "        vectorizer=None\n",
    "        if test:\n",
    "            vectorizer=CountVectorizer(vocabulary=self.vocabulary)\n",
    "            sparseMat=vectorizer.fit_transform(phrases)\n",
    "            return sparseMat.toarray()\n",
    "        else:\n",
    "            vectorizer=CountVectorizer()\n",
    "            sparseMat=vectorizer.fit_transform(phrases)\n",
    "            self.vocabulary=vectorizer.vocabulary_\n",
    "            return sparseMat.toarray()\n",
    "    \n",
    "    #Creates corresponding targets to each sentence.\n",
    "    def createTargets(self,df):\n",
    "        labels=[]\n",
    "        for i in range(len(df)):\n",
    "            labels.append(df.loc[i,\"emotions\"])\n",
    "        return labels\n",
    "    \n",
    "    #Creates and stores parameters as model attributes.\n",
    "    def fit(self):\n",
    "        priorProbabilities=self.createPriorProbabilities()\n",
    "        self.priorProbabilities=priorProbabilities\n",
    "        \n",
    "        posteriorProbabilities=self.createPosteriorProbabilities()\n",
    "        self.posteriorProbabilities=posteriorProbabilities\n",
    "        \n",
    "    #Creates prior probabilities of each emotion using multinoulli classification. \n",
    "    def createPriorProbabilities(self):\n",
    "        priorProbabilities=[0]*self.numTargets\n",
    "        for i in range(len(priorProbabilities)):\n",
    "            count=0\n",
    "            for target in self.targets:\n",
    "                if int(target)==i:\n",
    "                    count+=1\n",
    "            priorProbabilities[i]=count/self.numSamples\n",
    "        return priorProbabilities\n",
    "\n",
    "    #Creates posterior probabilites, the theta_(d,c).\n",
    "    def createPosteriorProbabilities(self):\n",
    "        posteriorProbabilites=[[0]*self.numFeatures for d in range(self.numTargets)] #CxD\n",
    "        \n",
    "        for c in range(self.numTargets):\n",
    "            posteriorProbabilites[c]=np.divide(self.wordFrequencies[c], self.totalWordFrequencies[c])\n",
    "        \n",
    "        return posteriorProbabilites\n",
    "    \n",
    "    #Predicts the labels for a test/validation dataframe.\n",
    "    def predict(self, testdf):\n",
    "        testMatrix=self.createSparseMat(testdf, test=1)\n",
    "        predictedTargets=[]\n",
    "        \n",
    "        for i in range(len(testMatrix)):\n",
    "            sentence=testMatrix[i]\n",
    "            probabilities=self.predictSentence(sentence)\n",
    "            predictedTargets.append(probabilities.index(max(probabilities)))\n",
    "            \n",
    "        return predictedTargets\n",
    "\n",
    "    #Gets accuracy for a test dataframe.\n",
    "    def getAcc(self, testdf):\n",
    "        testTargets=self.createTargets(testdf)\n",
    "        predictedTargets=self.predict(testdf)\n",
    "        \n",
    "        right=0\n",
    "        for i in range(len(testTargets)):\n",
    "            if testTargets[i]==predictedTargets[i]:\n",
    "                right+=1\n",
    "        return right/len(testTargets)\n",
    "    \n",
    "    def normalizationFactor(self,sentence):\n",
    "        sentenceSum=sum(sentence)\n",
    "        downstairs=1\n",
    "        for freq in sentence:\n",
    "            downstairs*=np.math.factorial(freq)\n",
    "        return np.math.factorial(sentenceSum)/downstairs\n",
    "    \n",
    "    #Returns probability of sentence being each class c.\n",
    "    def predictSentence(self, sentence):\n",
    "        probabilities=[0]*self.numTargets\n",
    "        \n",
    "        for c in range(self.numTargets):\n",
    "            res=1\n",
    "            res*=self.priorProbabilities[c] #Probability of it being class c.\n",
    "            for d in range(self.numFeatures):\n",
    "                factor=self.posteriorProbabilities[c][d]**(sentence[d])\n",
    "                res*=(factor) #Posterior probabilities.\n",
    "            probabilities[c]=res\n",
    "            \n",
    "        pSum=sum(probabilities)\n",
    "        for i in range(len(probabilities)):\n",
    "            probabilities[i]=probabilities[i]/pSum\n",
    "\n",
    "        return probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiyan Liu\\AppData\\Local\\Temp\\ipykernel_13520\\3833645821.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  probabilities[i]=probabilities[i]/pSum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.62\n",
      "Test accuracy is: 0.643\n"
     ]
    }
   ],
   "source": [
    "bayesModel=NaiveBayes(trainDF) #Training model\n",
    "testModel=NaiveBayes(testDF) #Easy access to processed data\n",
    "\n",
    "\n",
    "bayesModel.fit()\n",
    "\n",
    "print(\"Validation accuracy is:\", bayesModel.getAcc(validationDF))\n",
    "print(\"Test accuracy is:\", bayesModel.getAcc(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9265\n"
     ]
    }
   ],
   "source": [
    "#Importing pre-trained model without changing weights. \n",
    "bert = pipeline(\"sentiment-analysis\",model='bhadresh-savani/bert-base-uncased-emotion')\n",
    "\n",
    "right=0\n",
    "for i in range(len(testModel.sentences)):\n",
    "    sentence=testDF.loc[i,\"text\"]\n",
    "    prediction=bert(sentence)\n",
    "    predictedTarget=EMOTIONSDICT[prediction[0][\"label\"]]\n",
    "    if predictedTarget==testModel.targets[i]:\n",
    "        right+=1\n",
    "\n",
    "print(\"Accuracy is:\", right/len(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from pandatorch import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Creating the model and tokenizer.\n",
    "modelName='bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(modelName)\n",
    "model = BertForSequenceClassification.from_pretrained(modelName, num_labels=6)\n",
    "\n",
    "def tokenizeFunction(sentence):\n",
    "    return tokenizer(sentence, padding=\"max_length\", truncation=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, \"text\"]  # 'text' column\n",
    "        emotions = self.data.loc[idx, \"emotions\"]  # 'emotions' column\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs[\"input_ids\"].squeeze(),\n",
    "            'attention_mask': inputs[\"attention_mask\"].squeeze(),\n",
    "            'labels': int(emotions),\n",
    "        }\n",
    "\n",
    "#Tokenizing the training and text strings\n",
    "trainDataset=CustomDataset(trainDF, tokenizer)\n",
    "testDataset=CustomDataset(testDF, tokenizer)\n",
    "trainLoader=DataLoader(trainDataset, batch_size=16, shuffle=True)\n",
    "testLoader=DataLoader(testDataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [2:30:57<?, ?it/s]\n",
      "  0%|          | 1/3000 [2:28:23<7416:44:08, 8903.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shiyan Liu\\GitHub\\Comp-551-A3\\A3\\551A3.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trainingArgs\u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     eval_steps\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     args\u001b[39m=\u001b[39mtrainingArgs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtestDataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Shiyan%20Liu/GitHub/Comp-551-A3/A3/551A3.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inner_training_loop\u001b[39m(\n\u001b[0;32m   1563\u001b[0m     \u001b[39mself\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, resume_from_checkpoint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, trial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_keys_for_eval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m ):\n\u001b[1;32m-> 1565\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mfree_memory()\n\u001b[0;32m   1566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m   1567\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently training with a batch size of: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:3011\u001b[0m, in \u001b[0;36mAccelerator.free_memory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3009\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed_engine_wrapped \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3010\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 3011\u001b[0m release_memory()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\utils\\memory.py:56\u001b[0m, in \u001b[0;36mrelease_memory\u001b[1;34m(*objects)\u001b[0m\n\u001b[0;32m     54\u001b[0m     objects[i] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     55\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m---> 56\u001b[0m \u001b[39mif\u001b[39;00m is_xpu_available():\n\u001b[0;32m     57\u001b[0m     torch\u001b[39m.\u001b[39mxpu\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m     58\u001b[0m \u001b[39melif\u001b[39;00m is_npu_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Implementing pre-trained model, with changing weights.\n",
    "\n",
    "#Creating datasets for the training and text dataframes.\n",
    "trainingArgs= TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingArgs,\n",
    "    train_dataset=trainDataset,\n",
    "    #data_collator=lambda data: {\"input_ids\": data[0], \"attention_mask\": data[1], \"labels\": data[2]},\n",
    "    eval_dataset=testDataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
